# Dimensionality-Reduction

Dimensionality Reduction for Data Visualization: PCA vs TSNE vs UMAP vs LDA
Visualising a high-dimensional dataset in 2D and 3D using: PCA, TSNE, UMAP and LDA
What is Dimensionality Reduction?
Many Machine Learning problems involve thousands of features, having such a large number of features bring along many problems, the most important ones are:

Makes the training extremely slow
Makes it difficult to find a good solution
This is known as the curse of dimensionality and the Dimensionality Reduction is the process of reducing the number of features to the most relevant ones in simple terms.

Reducing the dimensionality does lose some information, however as most compressing processes it comes with some drawbacks, even though we get the training faster, we make the system perform slightly worse, but this is ok! “sometimes reducing the dimensionality can filter out some of the noise present and some of the unnecessary details”.

![image](https://user-images.githubusercontent.com/52233189/187133591-19bcdaf4-4991-4e9f-bce2-425cbd52a287.png)
